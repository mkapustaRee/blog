---
title: "Predicting residential property prices in Bratislava"
author: "Michal Kapusta"
date: "2018-10-23T21:13:14-05:00"
output: html_document
categories: ["R"]
tags: ["Webscraping", "Forecasting","Property prices"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
Sys.setlocale(category = "LC_ALL",locale ="de_DE")

library(tidyverse)
library(rvest)
library(leaflet)
library(rebus)
library(stringr)
library(janitor)
library(data.table)

df_ads <-  read_csv(file = "_files/list_of_ads [2018-08-08].csv")  %>% 
           filter(!str_detect(street,pattern = "Bratislava")) 

urls <-  read_csv(file = "_files/list_of_urls [2018-08-08].csv") %>%
         pull(value)

gps_street <-  read_csv(file = "_files/street_with_gps.csv")  %>%
                drop_na() %>%
                rename(street_long = street)

street <- gps_street$street_long %>%
                str_split(", ",simplify = T) %>%
                as.data.frame() %>%
                pull(V1) 

gps_street$street <- street

gps_street <- gps_street %>%
                mutate(street = as.character(street))

df_ads_wGPS <- df_ads %>%
                left_join(gps_street %>% select(street,lon,lat))

# geospatial data with distance calcs

df_transport <- fread("/Users/Michal/Desktop/R Projects/R projects/data_storage/transport.csv")
df_amenity   <- fread("/Users/Michal/Desktop/R Projects/R projects/data_storage/amenity.csv")




```

## The goal of the post

In this blogpost, I will be scraping data from property listing website in order to access real world data. After scraping and data cleaning steps I will build a predictive model that can predict house prices. Lets find out!

## Acquiring the real world data 

The property website **www.topreality.sk** is a good resource for the real house price listings. Also the website link generator is pretty straight forward. After filtering based on:

* the desired flat location (in this case all districts of Bratislava)
* number of bedrooms (1-4) 
* purchase instead of rent offers

the following link is generated:

<https://www.topreality.sk/vyhladavanie-nehnutelnosti-1.html?type%5B0%5D=108&type%5B1%5D=102&type%5B2%5D=103&type%5B3%5D=104&type%5B4%5D=105&form=1&obec=2%2C4%2C5%2C6%2C8%2C9%2C10%2C12%2C13%2C14%2C15%2C16%2C17%2C19%2C20%2C21%2C22&n_search=search&gpsPolygon=&searchType=string>

The first part of the url (*https://www.topreality.sk/vyhladavanie-nehnutelnosti-1.html*) is of particular interest since everything after this part are just filters. After plugging number *2* before the *.html* part it will load up second page of the search. 

<https://www.topreality.sk/vyhladavanie-nehnutelnosti-2.html?type%5B0%5D=108&type%5B1%5D=102&type%5B2%5D=103&type%5B3%5D=104&type%5B4%5D=105&form=1&obec=2%2C4%2C5%2C6%2C8%2C9%2C10%2C12%2C13%2C14%2C15%2C16%2C17%2C19%2C20%2C21%2C22&n_search=search&gpsPolygon=&searchType=string>

This information is essential, since it is the only variable that is changing in the url string. The icon to jump to the latest record (symbol **>>**) also contains information about the **last** url number. In this case it reveals 255 webpages are needed to list all the ads. Following url generator creates string of urls needed to scrape all the listings:

```{r url generator,eval = FALSE}
url <- "https://www.topreality.sk/vyhladavanie-nehnutelnosti-1.html?type%5B0%5D=108&type%5B1%5D=102&type%5B2%5D=103&type%5B3%5D=104&type%5B4%5D=105&form=1&obec=2%2C4%2C5%2C6%2C8%2C9%2C10%2C12%2C13%2C14%2C15%2C16%2C17%2C19%2C20%2C21%2C22&n_search=search&gpsPolygon=&searchType=string"

source_urls <- function(url) {
        library(rvest)
        library(stringr)
        url_length <- url %>%
                read_html() %>%
                html_nodes("a.last") %>%
                html_attr("href") 
        
        ptrn <- one_or_more(char_class(ASCII_ALNUM %R% "-"))
        
        url_length_num <- url_length %>%
                str_extract(pattern = ptrn) %>%
                str_replace(pattern = "vyhladavanie-nehnutelnosti-",
                            replacement = "") %>% 
                as.numeric()
        
        url_length_num <- url_length_num[1]
        
        url_length_seq <- 1:url_length_num
        
        url_seq <- str_c("https://www.topreality.sk/vyhladavanie-nehnutelnosti-",1:length(url_length_seq),".html?type%5B0%5D=108&type%5B1%5D=102&type%5B2%5D=103&type%5B3%5D=104&type%5B4%5D=105&form=1&obec=2%2C4%2C5%2C6%2C8%2C9%2C10%2C12%2C13%2C14%2C15%2C16%2C17%2C19%2C20%2C21%2C22&n_search=search&gpsPolygon=&searchType=string")
        
        print(paste0(length(url_length_seq)," - scraped urls"))
        
        return(url_seq)
        
        
}

urls        <- source_urls(url = url)

```

The result of the function **source_urls** is vector of websites (255 long) that are needed to scrape in order to get information about **all** the flats in the city of Bratislava. Here are the last three websites urls.

```{r show url,echo=F}
urls %>% tail(3) %>% knitr::kable()
```


Using the package **rvest** we can access the information displayed on the webpage. The following data-points are of our interest: 

* price of the flat 
* size of the flat in square meters 
* number of bedrooms
* street name
* district name
* id of the listing (part of the url leading to the listing)
* date of the listing

To scrape all the information we need to leverage the rvest, stringr, rebus and purrr packages.  

```{r scrape ads detail,eval=FALSE}
get_ads_data      <- function(url) {
        Sys.sleep(2)
        print(url)
        price           <- url %>% read_html() %>% html_nodes(".price strong") %>% html_text() 
        area            <- url %>% read_html() %>% html_nodes(".areas strong:nth-child(1)") %>% html_text()
        listed          <- url %>% read_html() %>% html_nodes(".date") %>% html_text()
        price_per_sqm   <- url %>% read_html() %>% html_nodes(".priceArea") %>% html_text()
        street          <- url %>% read_html() %>% html_nodes(".locality") %>% html_text()
        no_of_bedrooms  <- url %>% read_html() %>% html_nodes("li:nth-child(1) .noclick") %>% html_text()
        url_saved       <- url %>% read_html() %>% html_nodes("h2") %>% html_nodes("a") %>% html_attr("href") 
        
        ext_pattern <-    one_or_more(DIGIT) %R% ".html"
        id <- str_extract(url_saved,pattern = ext_pattern) %>% str_replace(pattern = ".html",replacement = "")
        
        
        
        final <- cbind(id,price,area,listed,price_per_sqm,street,no_of_bedrooms,url_saved) %>% as.tibble()
        
        return(final)
        
}
tidy_scraped_data <- function(data) {
        # vectors
        vct_price        <- data$price %>% str_split(pattern = ",",simplify = T) %>% as.tibble() %>% select(V1) %>% mutate_at(1, str_replace, pattern = " ",replacement = "")  %>% mutate_at(1, as.numeric) %>% pull(V1)
        vct_price_persqm <- data$price_per_sqm %>%  str_split(pattern = ",",simplify = T) %>% as.tibble() %>% select(V1)  %>% mutate_at(1, str_replace, pattern = " ",replacement = "") %>% mutate_at(1, as.numeric) %>% pull(V1)
        vct_no_bed       <- data$no_of_bedrooms %>%  str_split(pattern = " izb",simplify = T) %>% as.tibble() %>% select(V1)  %>% mutate_at(1, str_replace, pattern = " ",replacement = "") %>% mutate_at(1, as.numeric) %>% pull(V1)
        vct_district     <- data$street %>%  str_split(pattern = "\\(",simplify = T) %>% as.tibble() %>% select(V2)  %>% mutate_at(1, str_replace, pattern = "\\)",replacement = "")  %>% pull(V2)
        vct_street       <- data$street %>%  str_split(pattern = ",",simplify = T) 
        vct_street       <- vct_street[,1]
        vct_street       <- vct_street %>% str_remove(pattern = rebus::one_or_more(rebus::DIGIT)) %>% str_trim()
        
        #replace data
        data$price         <- vct_price
        data$price_per_sqm <- vct_price_persqm
        data$street        <- vct_street
        data$district      <- vct_district
        data$no_of_bedrooms <- vct_no_bed
        
        df_final <- data %>%  mutate_at(3, str_replace, pattern = " m2",replacement = "") %>% mutate_at(3, as.numeric)
        return(df_final)
}

df_ads_raw <- urls %>%
                map(safely(get_ads_data)) %>%
                map_df("result")

df_ads_tidy <- df_ads_raw %>%
                tidy_scraped_data()

```

The scraper generates following datatable:

```{r show ads,echo=F}
df_ads %>% head() %>% knitr::kable()
```

The database contains information about ca 2379 listings stored in consistent way. Perfect! 

## Exploratory Data Analysis

After the data is obtained we can proceed into the explanatory data analysis part. First lets look at the summary statistics.

```{r ads table}
df_ads %>% summary()

```

The table shows we have successfully scraped ca 2100 records. Approximately 100 records are incomplete and will be removed from the analysis. Few other observations: 

* Average price for flat in Bratislava is ca **173.000 Eur**
* Average size of the flat is **73** square meters
* Average price per square meter is **2478** Eur
* District variable needs to be converted to factors 

Now lets look at the data! The following charts show basic distribution of price and log(price). 

```{r histogram,warning=F,error=F, message=F}
df_ads  %>% drop_na() %>% 
        mutate(log_price = log10(price)) %>% 
        select(price, log_price) %>% 
        gather(Ratio, Value,1:2) %>% 
        ggplot(aes(Value)) +
        geom_histogram() +
        theme_minimal(base_family = "Verdana",base_size = 12)   + 
        facet_wrap(~Ratio, scales = "free") +
        theme(plot.title = element_text(face = "bold")) + 
        labs(title = "DISTRIBUTION OF APARTMENT PRICES IN BRATISLAVA")

```

The charts show that the majority of ads have price in range between 125.000 EUR - 200.000 EUR. The distribution is left-skewed an needs to be transformed using logarithm. The log10(price) ensures the distribution is normalised (more suitable for forecasting and human comprehention). For backward conversion use calculate for example: 5.5^10 = 316227 EUR. 

We would expect the prices vary across the five districts in Bratislava. The following chart confirms: 

```{r histogram per district,include=T, echo = F}
df_ads %>% 
        drop_na() %>% 
        ggplot(aes(district, log10(price) )) + 
        geom_jitter(alpha = .3) +
        theme_minimal() + 
        geom_boxplot(alpha = .3) + 
        labs(title = "BA I IS THE MOST EXPENSIVE? BA V THE MOST COMPETITIVE",
                subtitle = "Distribution of prices per city district") + 
        scale_color_brewer(palette = "RdYlGn") +
        theme(plot.title = element_text(face = "bold"))

```

* that prices are highest in Bratislava I - the old town district
* The most competitive priced is BA V. (small range between 25-75 % Quartile)  

Lets dig deeper into dataset to extract all the information. Lets look at price distributions within the district based on the number of beds. 

```{r histogram per district and per no_of_bedrooms, echo=F }

df_ads %>% 
        drop_na() %>% 
        ggplot(aes(district, log10(price), col = factor(no_of_bedrooms))) + 
        geom_jitter(alpha = .3) +
        theme_minimal() + 
        geom_boxplot(alpha = .3) + 
        labs(title = "PRICE INCREASES WITH THE # OF BEDROOM (BA V is exeption)",
                subtitle = "Distribution of prices per district and # of bedrooms") + 
        scale_color_brewer(palette = "RdYlGn")  +
        theme(legend.position = "bottom")+
        theme(plot.title = element_text(face = "bold"))

```

The transformation reveals: 

* Price increases by number of bedrooms (exception: BA V prices for 2 & 3 bedroom)
* Some of the ranges are very wide. Big differences occur within districts as well. 
* 4x bedroom flat in BA I cost on average (10 ^ 5.5 = ~400.000 EUR)
  
Next, lets look at the qualitative variable - **area** compared to the **price**

```{r area and price, echo = F}

df_ads %>% 
        drop_na() %>% 
        ggplot(aes(area, log10(price))) + 
        geom_jitter(alpha = .3) +
        theme_minimal() + 
        labs(title = "PRICE OF FLAT INCREASES WITH THE FLAT AREA",
                subtitle = "Price and area of the flat in relationship") + 
        scale_color_brewer(palette = "RdYlGn")  + 
        theme(legend.position = "bottom") + 
        facet_wrap(~district, scales = "free")   +
        geom_smooth(method = "lm",se =F) +
        theme(plot.title = element_text(face = "bold"))

```

The chart above shows that the flat prices and flat are are in linear relationship. This is to be expected. Also Bratislava IV looks clustered around the model line - suggesting stronger relationship in the market. Is this a mere coincidence or hidden insight? 

Next, lets fit a linear model using data at we assembled so far. We predict the price for the apartment using the traditional set of variables data scientists usually have at hand (mostly apartment metric and basic location variables)

```{r traditional model}

df_to_model <- df_ads  %>%
        drop_na() %>%
        select(-url_saved,-id,-listed, -price_per_sqm) %>% 
        mutate(no_of_bedrooms = no_of_bedrooms %>% factor(ordered = T,levels = c(1,2,3,4)),
               district = district %>% as.factor(),
               street = street %>% as.factor())

# split 

library(rsample)
library(caret)

set.seed(4595)
data_split <- initial_split(df_to_model, strata = "price")

df_train <- training(data_split)
df_test  <- testing(data_split)

cv_splits <- vfold_cv(df_train, v = 10, strata = "price")


# recipes
library(recipes)

model_recipe <- recipe(price ~ .  , data = df_train ) %>% 
                step_other(street, threshold = .0125) %>% 
                step_dummy(no_of_bedrooms, district, street)  %>%        # convert string of factors into columns with 0,1
                step_log(price, base = 10) %>%                  # log-transformation to normalise
                step_BoxCox(area)  # find suitable transformation for area var
                

# apply recipe on CV splits

cv_splits <- cv_splits %>%
        mutate(ads_rec = map(splits, prepper, recipe = model_recipe, retain = T))  # create CV splits

# apply lm function to CV splits

lm_fit_rec <- function(rec_obj, ...) {
        lm(..., data = juice(rec_obj))
        
}
 
cv_splits <- cv_splits %>% mutate(fits = map(ads_rec, lm_fit_rec, price ~.)) 


# generate predictions based on the splits
assess_predictions <- function(split_obj, rec_obj, mod_obj) {
  raw_data <- assessment(split_obj)
  proc_x <- bake(rec_obj, newdata = raw_data, all_predictors())
  bake(rec_obj, newdata = raw_data, everything()) %>%
    mutate(.fitted = predict(mod_obj, newdata = proc_x),
      .resid = price - .fitted,  
      .row = as.integer(split_obj, data = "assessment"))
}

cv_splits <- cv_splits %>%
  mutate(pred =  pmap( lst(split_obj = cv_splits$splits, rec_obj = cv_splits$ads_rec,mod_obj = cv_splits$fits),
        assess_predictions 
      )
  )
        

# measure performance
library(yardstick)

# Compute the summary statistics
map_df(cv_splits$pred, metrics, truth = price, estimate = .fitted) %>% 
  colMeans %>% knitr::kable()

```

The results shows that the regression Rsquared is around 0.66. This means around 66% of the price variance can be explained using the variables at hand. 

```{r model summary}
        cv_splits$fits[[1]] %>% summary()
```


## Adding alternative data sources

The analysis so far used only several variables to analyse price of apartments in Bratislava. Compared to other datasets this is low. Kaggle competitions usually have dozens of metrics to build model. This metrics are usually covering data like: floor number, construction date or fit-out quality. This additional data will ultimately lead to more accurate model. Since we dont have such informations we look to **alternative data sources** to augment our database with non-standard metrics. Our thesys revolves around old real estate saying: **Location,Location,Location**.

In the following chapter we will:

* source open-street-map data  
* use location features to enrich the database 
* calculate distance from apartments to location features 
* use the results in the predictive model  

### Source the data using Open-Street-Map

The geospatial databases are a great source of information. The two major sources are **Google Maps API** or the open-source option **Open-street Map API**. The option from google supports location feature extraction, but it cost money to retrieve thousands of records. 

First step is to enrich our database with geospatial data (longitude, latitude). For this purposes we will use the street address and city information. Our dataset contains the street name and flats are located in Bratislava.   

```{r add gps coordinate to the streets,eval=F}
library(ggmap)

geocodeAdddress <- function(address) {
  require(RJSONIO)
  print(address)
  url <- "http://maps.google.com/maps/api/geocode/json?address="
  url <- URLencode(paste(url, address, "&sensor=false", sep = ""))
  x <- fromJSON(url, simplify = FALSE)
  if (x$status == "OK") {
    out <- c(x$results[[1]]$geometry$location$lng,
             x$results[[1]]$geometry$location$lat)
  } else {
    out <- c(NA,NA)
  }
  Sys.sleep(0.2)  # API only allows 5 requests per second
  out
}


vct_adrres <- df_ads %>% 
        pull(street) %>% 
        paste0(", Bratislava") %>%
        data.frame() %>% 
        distinct() %>%
        pull() %>%
        as.character()

gps_coordinates <- map(.f = geocodeAdddress,
                       .x = vct_adrres) %>%
                        plyr::ldply()

```

The code above sources the geospatial information and yields following insights.

```{r show on the map, echo = F}
library(ggmap)

df_leaf <- df_ads_wGPS %>%
        group_by(street) %>%
        mutate(price_mean = mean(price_per_sqm, na.rm =T), 
               sample_size = n()) %>% 
        drop_na()

pal <- colorQuantile(palette = "viridis",domain = df_leaf$price_mean)


leaflet() %>%
        addCircles(data = df_leaf,
                   lng = ~lon,
                   lat = ~lat,
                   label = ~ street,
                   color  = ~pal(price_mean),
                   popup = ~paste("Street Name: ", df_leaf$street,"</br>",
                              "Mean Price:",df_leaf$price_mean %>% scales::dollar(),"</br>",
                              "Sample Size: ", df_leaf$sample_size)) %>%
        addProviderTiles("Stamen.TonerHybrid")  %>%
        addLegend(pal = pal, values = ~ price_mean,
                  data = df_leaf,
                  na.label = "N/A",
                  title = "Price distribution by Quartiles",opacity = 0.8) 


```

The former database can be charted using the longitude and latitude information. The map shows the streets in Bratislava grouped in four quartiles. The yellow color representing the 75-100% quartile means the prices are highest. From the map we can identify following facts: 

* Bratislava most expensive streets are located around city center (no suprise there :-) )
* Hurbanovo námestie street is displaying highest price at 6249 € per sq m
* The lowest price are recorded at street Vlčie Hrdlo, Čárskeho

### Attach the dataset to the apartment database

Now, lets look at the location features in the Bratislava. 

```{r display all geospatial,echo = F, warning=F}

df <- df_amenity %>% distinct(name,.keep_all = T)
pal <- colorFactor(palette = "Spectral",domain = df$feat)

leaflet() %>%
        addCircles(data = df,
                   lng = ~ feat_lon,
                   lat = ~ feat_lat,
                   label = ~ name,
                   color  = ~pal(df$feat),
                   popup = ~paste("Street Name: ", df$name,"</br>",
                              "Mean Price:",df$feat))  %>% 
         addProviderTiles("Stamen.TonerHybrid") %>% 
        addLegend(pal = pal, values = ~ feat,
                  data = df,
                  na.label = "N/A",
                  title = "Price distribution by Quartiles",opacity = 0.8) 


```


```{r display deal relevant geospatial data, echo=F, warning=F}


df <- df_amenity %>% filter(street_long %in% c("Mariánska, Bratislava", "Pečnianska, Bratislava"))

df_sliced <- df %>% split(.$street_long) %>% map_df(slice, 1)


pal <- colorFactor(palette = "Spectral",domain = df$feat)

leaflet() %>%
        addCircles(data = df,
                   lng = ~ feat_lon,
                   lat = ~ feat_lat,
                   label = ~ name,
                   color  = ~pal(df$feat),
                   popup = ~paste("Street Name: ", df$name,"</br>",
                              "Mean Price:",df$feat))  %>%
        addMarkers(data = df_sliced, 
                   lng = ~ lon,
                   lat = ~ lat,
                   label = ~ street_long) %>% 
         addProviderTiles("Stamen.TonerHybrid") %>% 
        addLegend(pal = pal, values = ~ feat,
                  data = df,
                  na.label = "N/A",
                  title = "Price distribution by Quartiles",opacity = 0.8) 
        
        

```


```{r histogram of distances}

df_hist      <- df_amenity %>% group_by(street_long,feat) %>% summarise(min = min(distance))
df_hist_mean <- df_amenity %>% group_by(street_long,feat) %>% summarise(min = min(distance)) %>% ungroup() %>% group_by(feat) %>% mutate(mean = median(min, na.rm = T))

ggplot() +
        geom_histogram(data = df_hist,aes(min)) + 
        facet_wrap(~feat, scales = "free") + 
        geom_vline(aes(xintercept = mean), data  = df_hist_mean ,linetype = 1, color = "red") + 
        theme_minimal() +
        labs(title)

```


```{r extended database}

check_if_na <- function (x) {
    x[is.na(x)] <- 0
    return(x)
}

scored_amenity <- df_amenity %>%
                        split(.$cat) %>%
                        map_df(mutate, pct = ntile(-distance, 5)) %>%
                        split(.$street_long) %>%
                        map(group_by, cat) %>%
                        map_df(mutate, score = sum(pct,na.rm = T)) %>%
                        select(street_long, cat, score) %>%
                        distinct() %>% 
                        spread(cat, score)

scored_transport <- df_transport %>%
                        split(.$cat) %>%
                        map_df(mutate, pct = ntile(-distance, 5)) %>%
                        split(.$street_long) %>%
                        map(group_by, cat) %>%
                        map_df(mutate, score = sum(pct,na.rm = T)) %>%
                        select(street_long, cat, score) %>%
                        distinct() %>% 
                        spread(cat, score)



df_model <- df_ads_wGPS %>%
        drop_na() %>% 
        mutate(street_long = paste0(street, ", Bratislava")) %>%
        left_join(scored_amenity, by = "street_long") %>%
        left_join(scored_transport, by = "street_long") %>%
        select(id, price, area, street, district,no_of_bedrooms, contains("amenity"),contains("public"),lon, lat) %>% 
        mutate(no_of_bedrooms = no_of_bedrooms %>% factor(ordered = T, levels = c(1,2,3,4)),
               district = district %>% factor(),
               street = street %>% factor()) %>% 
        mutate_if(is.numeric, check_if_na) %>% 
        drop_na() 

df_model %>% summary()

```

### Model

```{r linear model, eval = F}
# split 

library(rsample)
library(caret)


set.seed(4595)
data_split <- initial_split(df_model, strata = "price")

df_train <- training(data_split)
df_test  <- testing(data_split)

cv_splits <- vfold_cv(df_train, v = 10, strata = "price")


# recipes
library(recipes)

model_recipe_gps <- recipe(price ~ .  , data = df_train ) %>% 
                        step_dummy(no_of_bedrooms,district)  %>% 
                        step_log(area, price, base = 10) %>%  
                        step_other(street, threshold = 0.05) %>% 
                        step_interact(~ contains("no_of_bedrooms"):area) 
                        

# apply recipe on all splits

cv_splits <- cv_splits %>% mutate(ads_rec = map(splits, prepper, recipe = model_recipe_gps, retain = T))


# apply lm function to all splits

lm_fit_rec <- function(rec_obj, ...) {
        lm(..., data = juice(rec_obj))
        
}
 
cv_splits <- cv_splits %>% mutate(fits = map(ads_rec, lm_fit_rec, price ~.))


# generate predictions based on the splits
assess_predictions <- function(split_obj, rec_obj, mod_obj) {
  raw_data <- assessment(split_obj)
  proc_x <- bake(rec_obj, newdata = raw_data, all_predictors())
  bake(rec_obj, newdata = raw_data, everything()) %>%
    mutate(.fitted = predict(mod_obj, newdata = proc_x),
      .resid = price - .fitted,  
      .row = as.integer(split_obj, data = "assessment"))
}

cv_splits <- cv_splits %>%
  mutate(
    pred = 
      pmap(
        lst(split_obj = cv_splits$splits, 
            rec_obj = cv_splits$ads_rec, 
            mod_obj = cv_splits$fits),
        assess_predictions 
      )
  )
        

# measure performance
library(yardstick)

# Compute the summary statistics
map_df(cv_splits$pred, metrics, truth = price, estimate = .fitted) %>% 
  colMeans

assess_pred <- bind_rows(cv_splits$pred) %>%
  mutate(price = 10^price %>% round(digits = 0),
         .fitted = 10^.fitted %>% round(digits = 0)) 

assess_pred %>% 
        ggplot(
       aes(x = price, y = .fitted)) + 
  geom_abline(lty = 2) + 
  geom_point(alpha = .4)  + 
  geom_smooth(se = FALSE, col = "red") + 
        scale_x_continuous(labels = scales::dollar_format(suffix = "€", prefix = "")) + 
        scale_y_continuous(labels = scales::dollar_format(suffix = "€", prefix = "")) + 
        theme_minimal()


```


```{r knn, eval=F}

model_recipe_norm <- recipe(price ~ .  , data = df_train ) %>% 
                        step_dummy(no_of_bedrooms,district)  %>% 
                        step_log(price, base = 10) %>%  
                        step_BoxCox(area)  %>% 
                        step_other(street, threshold = .03) %>% 
                        step_center(all_numeric(),-price) %>% 
                        step_scale(all_numeric(),-price) %>% 
                        step_interact(~ contains("no_of_bedrooms"):area) %>% 
                        step_bs(lon, lat, options = list(df = 5))  


converted_resamples <-  rsample2caret(cv_splits)

ctrl <- trainControl(method = "cv",  savePredictions = "final")
ctrl$index <- converted_resamples$index
ctrl$indexOut <- converted_resamples$indexOut

knn_grid <- expand.grid(
  kmax = 1:9,
  distance = 2,
  kernel = c("rectangular", "triangular", "gaussian")
  )

knn_fit <- train(
  model_recipe_norm, data = df_train,
  method = "kknn", 
  tuneGrid = knn_grid,
  trControl = ctrl
)

getTrainPerf(knn_fit)

ggplot(knn_fit) + theme_minimal()


knn_pred <- knn_fit$pred %>%
  mutate(price = 10^obs %>% round(digits = 0) ,
         .fitted = 10^pred %>% round(digits = 0))

ggplot(knn_pred,
       aes(x = price, y = .fitted)) + 
  geom_abline(lty = 2) + 
  geom_point(alpha = .4)  + 
  geom_smooth(se = FALSE, col = "red")  %>% 
  scale_x_continuous(labels = scales::comma)

```



