---
title: "Predicting residential property prices in Bratislava using recipes - Linear regression (part I)"
author: "Michal Kapusta"
date: "2018-10-23T21:13:14-05:00"
output: html_document
categories: ["R"]
tags: ["Webscraping", "Forecasting","Property prices", "Recipes","Rsample"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
Sys.setlocale(category = "LC_ALL",locale ="de_DE")

library(tidyverse)
library(rvest)
library(leaflet)
library(rebus)
library(stringr)
library(janitor)
library(data.table)

df_ads <-  read_csv(file = "_files/list_of_ads [2018-08-08].csv")  %>% 
           filter(!str_detect(street,pattern = "Bratislava")) 

urls <-  read_csv(file = "_files/list_of_urls [2018-08-08].csv") %>%
         pull(value)

gps_street <-  read_csv(file = "_files/street_with_gps.csv")  %>%
                drop_na() %>%
                rename(street_long = street)

street <- gps_street$street_long %>%
                str_split(", ",simplify = T) %>%
                as.data.frame() %>%
                pull(V1) 

gps_street$street <- street

gps_street <- gps_street %>%
                mutate(street = as.character(street))

df_ads_wGPS <- df_ads %>%
                left_join(gps_street %>% select(street,lon,lat))

# geospatial data with distance calcs

df_transport <- fread("/Users/Michal/Desktop/R Projects/R projects/data_storage/transport.csv")
df_amenity   <- fread("/Users/Michal/Desktop/R Projects/R projects/data_storage/amenity.csv")




```

## The goal of the post

In this blogpost, i will be predicting the residential prices in the city of Bratislava (Slovakian capital). In order to access real world data I will be scraping data from property listing website. After scraping and data cleaning steps, I will build a predictive model that can predict house prices. 

## Acquiring the real world data 

The property website **www.topreality.sk** is a good resource for the appartment price listings. in addition, the website link generator is straight forward. After filtering based on:

* the desired flat location (in this case all districts of Bratislava)
* number of bedrooms (1-4) 
* purchase instead of rent offers

the following link is generated:

<https://www.topreality.sk/vyhladavanie-nehnutelnosti-1.html?type%5B0%5D=108&type%5B1%5D=102&type%5B2%5D=103&type%5B3%5D=104&type%5B4%5D=105&form=1&obec=2%2C4%2C5%2C6%2C8%2C9%2C10%2C12%2C13%2C14%2C15%2C16%2C17%2C19%2C20%2C21%2C22&n_search=search&gpsPolygon=&searchType=string>

The first part of the url (*https://www.topreality.sk/vyhladavanie-nehnutelnosti-1.html*) is of particular interest since everything after this part are just filters. After plugging number *2* before the *.html* part it will load up second page of the search. 

<https://www.topreality.sk/vyhladavanie-nehnutelnosti-2.html?type%5B0%5D=108&type%5B1%5D=102&type%5B2%5D=103&type%5B3%5D=104&type%5B4%5D=105&form=1&obec=2%2C4%2C5%2C6%2C8%2C9%2C10%2C12%2C13%2C14%2C15%2C16%2C17%2C19%2C20%2C21%2C22&n_search=search&gpsPolygon=&searchType=string>

This information is essential, since it is the only variable that is changing in the url string. The icon to jump to the latest record (symbol **>>**) also contains information about the **last** url number. In this case it reveals 255 webpages are needed to list all the ads. Following url generator creates string of urls needed to scrape all the listings:

```{r url generator,eval = FALSE}
url <- "https://www.topreality.sk/vyhladavanie-nehnutelnosti-1.html?type%5B0%5D=108&type%5B1%5D=102&type%5B2%5D=103&type%5B3%5D=104&type%5B4%5D=105&form=1&obec=2%2C4%2C5%2C6%2C8%2C9%2C10%2C12%2C13%2C14%2C15%2C16%2C17%2C19%2C20%2C21%2C22&n_search=search&gpsPolygon=&searchType=string"

source_urls <- function(url) {
        library(rvest)
        library(stringr)
        url_length <- url %>%
                read_html() %>%
                html_nodes("a.last") %>%
                html_attr("href") 
        
        ptrn <- one_or_more(char_class(ASCII_ALNUM %R% "-"))
        
        url_length_num <- url_length %>%
                str_extract(pattern = ptrn) %>%
                str_replace(pattern = "vyhladavanie-nehnutelnosti-",
                            replacement = "") %>% 
                as.numeric()
        
        url_length_num <- url_length_num[1]
        
        url_length_seq <- 1:url_length_num
        
        url_seq <- str_c("https://www.topreality.sk/vyhladavanie-nehnutelnosti-",1:length(url_length_seq),".html?type%5B0%5D=108&type%5B1%5D=102&type%5B2%5D=103&type%5B3%5D=104&type%5B4%5D=105&form=1&obec=2%2C4%2C5%2C6%2C8%2C9%2C10%2C12%2C13%2C14%2C15%2C16%2C17%2C19%2C20%2C21%2C22&n_search=search&gpsPolygon=&searchType=string")
        
        print(paste0(length(url_length_seq)," - scraped urls"))
        
        return(url_seq)
        
        
}

urls        <- source_urls(url = url)

```

The result of the function **source_urls** is vector of websites (255 long) that are needed to scrape in order to get information about **all** the flats in the city of Bratislava. Here are the last three websites urls.

```{r show url,echo=F}
urls %>% tail(3) %>% knitr::kable()
```


Using the package **rvest** we can access the information displayed on the webpage. The following data-points are of our interest: 

* price of the flat 
* size of the flat in square meters 
* number of bedrooms
* street name
* district name
* id of the listing (part of the url leading to the listing)
* date of the listing

To scrape all the information we need to leverage the rvest, stringr, rebus and purrr packages.  

```{r scrape ads detail,eval=FALSE}
get_ads_data      <- function(url) {
        Sys.sleep(2)
        print(url)
        price           <- url %>% read_html() %>% html_nodes(".price strong") %>% html_text() 
        area            <- url %>% read_html() %>% html_nodes(".areas strong:nth-child(1)") %>% html_text()
        listed          <- url %>% read_html() %>% html_nodes(".date") %>% html_text()
        price_per_sqm   <- url %>% read_html() %>% html_nodes(".priceArea") %>% html_text()
        street          <- url %>% read_html() %>% html_nodes(".locality") %>% html_text()
        no_of_bedrooms  <- url %>% read_html() %>% html_nodes("li:nth-child(1) .noclick") %>% html_text()
        url_saved       <- url %>% read_html() %>% html_nodes("h2") %>% html_nodes("a") %>% html_attr("href") 
        
        ext_pattern <-    one_or_more(DIGIT) %R% ".html"
        id <- str_extract(url_saved,pattern = ext_pattern) %>% str_replace(pattern = ".html",replacement = "")
        
        
        
        final <- cbind(id,price,area,listed,price_per_sqm,street,no_of_bedrooms,url_saved) %>% as.tibble()
        
        return(final)
        
}
tidy_scraped_data <- function(data) {
        # vectors
        vct_price        <- data$price %>% str_split(pattern = ",",simplify = T) %>% as.tibble() %>% select(V1) %>% mutate_at(1, str_replace, pattern = " ",replacement = "")  %>% mutate_at(1, as.numeric) %>% pull(V1)
        vct_price_persqm <- data$price_per_sqm %>%  str_split(pattern = ",",simplify = T) %>% as.tibble() %>% select(V1)  %>% mutate_at(1, str_replace, pattern = " ",replacement = "") %>% mutate_at(1, as.numeric) %>% pull(V1)
        vct_no_bed       <- data$no_of_bedrooms %>%  str_split(pattern = " izb",simplify = T) %>% as.tibble() %>% select(V1)  %>% mutate_at(1, str_replace, pattern = " ",replacement = "") %>% mutate_at(1, as.numeric) %>% pull(V1)
        vct_district     <- data$street %>%  str_split(pattern = "\\(",simplify = T) %>% as.tibble() %>% select(V2)  %>% mutate_at(1, str_replace, pattern = "\\)",replacement = "")  %>% pull(V2)
        vct_street       <- data$street %>%  str_split(pattern = ",",simplify = T) 
        vct_street       <- vct_street[,1]
        vct_street       <- vct_street %>% str_remove(pattern = rebus::one_or_more(rebus::DIGIT)) %>% str_trim()
        
        #replace data
        data$price         <- vct_price
        data$price_per_sqm <- vct_price_persqm
        data$street        <- vct_street
        data$district      <- vct_district
        data$no_of_bedrooms <- vct_no_bed
        
        df_final <- data %>%  mutate_at(3, str_replace, pattern = " m2",replacement = "") %>% mutate_at(3, as.numeric)
        return(df_final)
}

df_ads_raw <- urls %>%
                map(safely(get_ads_data)) %>%
                map_df("result")

df_ads_tidy <- df_ads_raw %>%
                tidy_scraped_data()

```

The scraper generates following datatable:

```{r show ads,echo=F}
df_ads %>% head() %>% knitr::kable()
```

The database contains information about ca 2379 listings stored in consistent way. Perfect! 

## Exploratory Data Analysis

After the data is obtained we can proceed into the explanatory data analysis part. First lets look at the summary statistics.

```{r ads table, warning=F}
library(skimr)
df_ads %>% skimr::skim_to_list()

```

The table shows we have successfully scraped ca 2100 records. Approximately 100 records are incomplete and will be removed from the analysis. Few other observations: 

* Average price for flat in Bratislava is ca **173.000 Eur**
* Average size of the flat is **73** square meters
* Average price per square meter is **2478** Eur
* District variable needs to be converted to factors 

Now lets look at the data! The following charts show basic distribution of price and log(price). 

```{r histogram,warning=F,error=F, message=F}
df_ads  %>% drop_na() %>% 
        mutate(log_price = log10(price)) %>% 
        select(price, log_price) %>% 
        gather(Ratio, Value,1:2) %>% 
        ggplot(aes(Value)) +
        geom_histogram() +
        theme_minimal(base_family = "Verdana",base_size = 12)   + 
        facet_wrap(~Ratio, scales = "free") +
        theme(plot.title = element_text(face = "bold")) + 
        labs(title = "DISTRIBUTION OF APARTMENT PRICES IN BRATISLAVA")

```

The charts show that the majority of ads have price in range between 125.000 EUR - 200.000 EUR. The distribution is right-skewed an needs to be transformed using logarithm. The log10(price) ensures the distribution is normalised (more suitable for forecasting and human comprehension). For backward conversion use calculate for example: 10^5.5 = 316227 EUR. 

Now, we would expect the prices vary across the five districts in Bratislava. Let's find out! 

```{r histogram per district,include=T, echo = F, warning=F}
df_ads %>% 
        drop_na() %>% 
        ggplot(aes(district, log10(price) )) + 
        geom_jitter(alpha = .3) +
        theme_minimal() + 
        geom_boxplot(alpha = .3) + 
        labs(title = "BA I IS THE MOST EXPENSIVE? BA V THE MOST COMPETITIVE",
                subtitle = "Distribution of prices per city district") + 
        scale_color_brewer(palette = "RdYlGn") +
        theme(plot.title = element_text(face = "bold"))

```

* that prices are highest in Bratislava I - the old town district
* BA I median price is around 5.45 or 281.000€
* The most competitive priced is BA V. (small range between 25-75 % Quartile)  

Lets dig deeper into dataset to extract all the information. Lets look at price distributions within the district based on the number of beds. 

```{r histogram per district and per no_of_bedrooms, echo=F, warning=F }

df_ads %>% 
        drop_na() %>% 
        ggplot(aes(district, log10(price), col = factor(no_of_bedrooms))) + 
        geom_jitter(alpha = .3) +
        theme_minimal() + 
        geom_boxplot(alpha = .3) + 
        labs(title = "PRICE INCREASES WITH THE # OF BEDROOM (BA V is exeption)",
                subtitle = "Distribution of prices per district and # of bedrooms") + 
        scale_color_brewer(palette = "RdYlGn")  +
        theme(legend.position = "bottom")+
        theme(plot.title = element_text(face = "bold"))

```

The transformation reveals: 

* Price increases by number of bedrooms (exception: BA V prices for 2 & 3 bedroom)
* Some of the ranges are very wide. Big differences occur within districts as well. 
* Most costly category is a 4x bedroom flat in BA I costing ~400.000 EUR (10 ^ 5.6)
  
Next, lets look at the qualitative variable - **area** compared to the **price**

```{r area and price, echo = F, warning=F}

df_ads %>% 
        drop_na() %>% 
        ggplot(aes(area, log10(price))) + 
        geom_jitter(alpha = .3) +
        theme_minimal() + 
        labs(title = "PRICE OF FLAT INCREASES WITH THE FLAT AREA",
                subtitle = "Price and area of the flat in relationship") + 
        scale_color_brewer(palette = "RdYlGn")  + 
        theme(legend.position = "bottom") + 
        facet_wrap(~district, scales = "free")   +
        geom_smooth(method = "lm",se =F) +
        theme(plot.title = element_text(face = "bold"))

```

The chart above shows that the flat prices is in linear relationship with the flat area. This is to be expected. Also Bratislava IV looks clustered around the model line - suggesting stronger relationship in the market. Is this a mere coincidence or hidden insight? 

Next, lets fit a linear model using data at we assembled so far. We will predict the price for the apartments using the traditional set of variables data scientists usually have at hand. The following chunk of code is a direct copy of Max Kuhn R code presented at UseR 2018 workshop <https://github.com/topepo/user2018>. 

```{r traditional model, warning=F, message=F}
set.seed(4595)

df_to_model <- df_ads  %>%
        drop_na() %>%
        select(-url_saved,-listed, -price_per_sqm) %>% 
        mutate(no_of_bedrooms = no_of_bedrooms %>% as.factor(),
               district = district %>% as.factor(),
               street = street %>% as.factor())

# split 

library(rsample)
library(caret)

set.seed(4595)
data_split <- initial_split(df_to_model, strata = "price")

df_train <- training(data_split)
df_test  <- testing(data_split)

cv_splits <- vfold_cv(df_train, v = 10, strata = "price")


# recipes
library(recipes)

model_recipe <- recipe(price ~ . - id, data = df_train ) %>% 
                step_other(street, threshold = .0125) %>% 
                step_dummy(street, district, no_of_bedrooms)  %>%      
                step_log(price, base = 10) %>%                         
                step_BoxCox(area) 

# create processed databases
preped_model <- model_recipe %>% prep()

x_train_processed_tbl <- bake(preped_model, df_train) 
x_test_processed_tbl  <- bake(preped_model, df_test)


# apply recipe on CV splits

cv_splits <- cv_splits %>%
        mutate(ads_rec = map(splits, prepper, recipe = model_recipe, retain = T))  # create CV splits

# apply lm function to CV splits

lm_fit_rec <- function(rec_obj, ...) {
        lm(..., data = juice(rec_obj))
        
}
 
cv_splits <- cv_splits %>% mutate(fits = map(ads_rec, lm_fit_rec, price ~.)) 


# generate predictions based on the splits
assess_predictions <- function(split_obj, rec_obj, mod_obj) {
  raw_data <- assessment(split_obj)
  proc_x <- bake(rec_obj, newdata = raw_data, all_predictors())
  bake(rec_obj, newdata = raw_data, everything()) %>%
    mutate(.fitted = predict(mod_obj, newdata = proc_x),
      .resid = price - .fitted,  
      .row = as.integer(split_obj, data = "assessment"))
}

cv_splits <- cv_splits %>%
  mutate(pred =  pmap( lst(split_obj = cv_splits$splits, rec_obj = cv_splits$ads_rec,mod_obj = cv_splits$fits),
        assess_predictions 
      )
  )
        

# measure performance
library(yardstick)

# Compute the summary statistics
map_df(cv_splits$pred, metrics, truth = price, estimate = .fitted) %>% 
  colMeans %>% 
  knitr::kable()

```

The code above is broadly doing following steps: 

* splitting the datasets into 10 equal-sized subsets (cross-validation)
* recipe step 1: collapse infrequent street names into "other"
* recipe step 2: create binary column (0 or 1) for each of the factor variables **no_of_bedrooms, district, street**
* recipe step 3: log transformation of price
* recipe step 4: Box-Cox transformation of area variable

After splitting the dataset and performing The results shows that the regression Rsquared is around **0.63**. This means around 66% of the price variance can be explained using the variables at hand. 

The detailed breakdown of the statistical model can be seen here:

```{r model summary, warning=F, render = F}
        cv_splits$fits[[1]] %>% summary()
```

Now let's visualise the dataset 

```{r model visualisation, warning=F}
cv_splits %>%
        unnest(pred) %>% 
        mutate(pred_accuracy = ntile(.resid, 5)) %>% 
        ggplot(aes(price, .fitted, col = as.factor(pred_accuracy))) +
        geom_point() + 
        geom_abline() + 
        scale_color_brewer(palette = "RdYlGn") +
        theme_minimal() +
        labs(title = "MODEL ACCURACY: Fitted vs Real Prices",
                subtitle = "Price and prediction in a relationship") + 
        theme(legend.position = "bottom") + 
        theme(plot.title = element_text(face = "bold")) +
        geom_smooth(se = F, color = "red")
 
```

The points around the line (3 pred_accuracy) are the most accurate estimates of the model - where residual values are small. Other colors are showing overvalued prices (5 pred_accuracy) and undervalues (1 pred_accuracy). The red line indicates that low prices are overpredicted and high prices are underpredicted by the linear model.   

Even better understanding offers a interactive map of Bratislava with original prices and the linear model results: 

```{r show leaflet of properties, warning=F, echo=F}
library(ggmap)

df_leaf <- cv_splits %>%
        unnest(pred) %>% 
        select(-id) %>% 
        mutate(pred_accuracy = ntile(.resid, 5) %>% as.integer(),
               price_eur = 10 ^ price,
               model_eur = 10 ^ .fitted, 
               diff = (price_eur - model_eur) %>% round(digits = 0),
               id = id1)   %>%
        select(-price,-id1) %>% 
        left_join(df_ads_wGPS, by = "id") %>% 
        drop_na() %>%
        group_by(street) %>%
        mutate(price_mean = mean(price_per_sqm, na.rm =T), 
               sample_size = n())


pal <- colorFactor(palette = "viridis",domain = df_leaf$pred_accuracy)


leaflet() %>%
        addCircleMarkers(data = df_leaf,
                   lng = ~lon,
                   lat = ~lat,fillOpacity = .7,
                   label = ~ street,
                   color  = ~pal(pred_accuracy),
                   popup = ~paste("Street Name: ", df_leaf$street,"</br>",
                              "Predicted Price (EUR):",df_leaf$model_eur %>% scales::comma(),"</br>",
                              "Offered Size (EUR): ", df_leaf$price_eur  %>% scales::comma(),"</br>",
                              "Difference (EUR): " , df_leaf$diff %>% scales::comma(),"</br>",
                              "Sample size (n): ", df_leaf$sample_size  %>%  scales::comma() )) %>%
        addProviderTiles("Stamen.TonerHybrid")  %>%
        addLegend(pal = pal,
                  values = ~pred_accuracy,
                  data = df_leaf,
                  na.label = "N/A",
                  title = ~paste0("Undervalued = 1","</br>",
                                 "Overvalued  = 5" 
                                 ),opacity = 0.8)

 
```

Finally, here are the 10 most undervalued streets in Slovakian capital:

```{r top 10 undervalued streets, warning=F, echo = F}

df_leaf %>% 
        arrange(diff) %>% 
        head(10) %>% 
        ggplot(aes(reorder(street, -diff),diff, fill = district)) +
        geom_col() +
        coord_flip() + 
        scale_y_continuous(labels = scales::dollar_format(prefix = "",suffix = "€"))  + 
        theme_minimal() + 
        scale_fill_brewer(palette = "Spectral") + 
        labs(title = "MODEL PREDICTION: Most undervalued streets in BA",
                subtitle = "Difference between offered price and model estimate",
                x = "") + 
        theme(legend.position = "bottom") + 
        theme(plot.title = element_text(face = "bold"))

```

## Further work and final considerations

In summary, we have successfully scraped and analysed residential property market in Bratislava. With ca 2.000 ads we found linear model that can explain ca 63% of the price variance. For real world application this result is too weak. The problem needs more advanced statistical models and possibly more data (distance to city center, number of transportation options. etc) to drive the accuracy even higher. 
